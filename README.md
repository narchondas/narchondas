- üëã Hi, I‚Äôm @narchondas
- üëÄ I‚Äôm interested in Artificial Intelligence (AI) and in better regulation (evaluation, impact assessment and stakeholder consultation)
- üå± I‚Äôm currently learning the advantages and pitfalls of the machine learning methods and algorithms
- üíûÔ∏è I‚Äôm looking to collaborate on AI in the public sector and on code writing
- üì´ How to reach me: by email, via [LinkedIn](http://www.linkedin.com/in/nikos-archontas-5a9a90234)
- üòÑ Pronouns: he/his
- ‚ö° Fun fact: I love swimming in the sea for more than an hour so, a couple of times, my friends were so worried that they wanted to call the coast guards as they assumed something bad had happened to me
  
# E-Portfolio of Nikos Archontas¬†
![f3](https://github.com/user-attachments/assets/c992b5f9-9741-437c-9b72-96b4a930178f)

## MSc. Artificial Intelligence Cohort July 2024

## Professional
I am an expert in Better Regulation and in particular in Stakeholder Engagement. Since 2018, I have been working on European Commission's evaluations, impact assessments and stakeholder consultations. 

As Bellevue programme fellow in the SGMAP (2016-2017), I had the opportunity to work in the French public administration and deal with a different and challenging administrative and e-government context. 

As team leader and project manager, working in the Greek Ministry of Interior (2002-2016), I had the opportunity to lead several political and technical structural reform and e-government projects aiming at the modernisation of the Greek public administration. 

The administrative reform process is a magnificent ‚Äòjourney‚Äô full of excitement, setbacks but also of great achievements which better the lives of the European citizens.

# University of Essex Learning Experience

## My Portfolio  
Welcome to my portfolio for the MSc in Artificial Intelligence! Here, you will find information about my studies, experience, and projects.  

## Induction Computing

### This module aims to: 

*Provide an introduction to KOL and the University of Essex Online, its virtual learning environment (VLE), Moodle, and the available library resources.  Familiarise students with the resources available in the Study Skills Hub. Guide students on how to best engage in online learning (learning communities, organising learning).

*Introduce students to the ideas of reflective learning and critical thinking.

*Refresh students‚Äô knowledge of referencing and the avoidance of academic dishonesty.

*Introduce the skills required for successful completion of future modules (including academic writing and research skills).

### In this module I shall:

*Demonstrate the technical and personal requirements of online learning.

*Demonstrate competencies in the use of the tools available to online learners.

*Demonstrate academic skills, e.g. essay/report writing, research techniques, referencing, reflective learning and critical thinking at a level commensurate with Postgraduate study.

### Induction Assignement

**Discuss the importance of a postgraduate degree in the Computer Science field.**

The education programme I am undertaking at the University of Essex Online is the MSc in Artificial Intelligence.  Even though I come from a non-STEM background, thanks to my professional experience, I came to realise how important ICT advancements are for most of the industries and the public sector, in general.   The Artificial Intelligence (AI) is at present continuously being tested and used in a variety of industries.  For example, Netflix produces hundreds of millions of film recommendations every day to subscribers, Amazon makes instant book/product suggestions depending on the search results and orders we have proceeded to.  Thanks to the strong computational power and the possibility to rapidly analyse datasets, AI is also helping to timely detect medical diseases and facilitate citizens to electronically submit documents in the public authorities by means of chatbots. 

Hern A. & Milmo D. (2024) argued that to make computers more intelligent, so they be able to resolve complicated problems, we must teach them learning patterns on how to imitate human behaviour.  Given that computers cannot independently think, they should be trained to analyse large amounts of data and then to make conclusions and inferences.  This is how AI is generally defined by academia and researchers.

I am currently working in the European Commission (EC), as a Stakeholder consultation expert and Horizontal coordinator of the Better Regulation agenda.  The reasons why I decided to dive into the world of Science and in particular in that of the AI are multiple.  AI will definitely shape the future of humanity and more concretely our life as consumers/citizens/voters/workers/patients or just humans.  

According to Deloitte (2018), ‚Äòit is the merging of physical and digital realms, which not only creates interconnectedness, but also enables people to make more informed decisions‚Äô.  What will be the consequence of travelling on self-driving cars?  To what extent the life expectancy of an inhabitant on a small Aegean island will be determined by the trustworthy and reliable use of AI when detecting medical diseases?  How can AI help us interact with the public sector totally eclipsing the physical visit to public authorities?  What are the hidden concerns and risks of using uncontrolled AI-enabled tools?  How can we make sure that we address the potential mismanagement of the technology, and we mitigate the risks for the humanity? Who gets the ownership of a generative-AI product, whether it be an image, video or text, and disposes of their intellectual property and copyright: the ‚Äòprompter‚Äô or the AI system provider? What would be the implications for ordinary people if their personal data are leaked and processed without their consent? I am expecting that all these pertinent questions will undoubtedly be answered or at least touched upon during the educational programme.

The EU is one of the pioneers among the world regulators which has already introduced a framework as for the development of the AI.  According to its strategy, it is keen ‚Äò[to] build institutional and operational capacity to ensure the development and use of trustworthy AI technologies‚Äô (European Commission, 2024).  AI-enabled tools are currently being used, among others: i. to translate large documents into 24 official EU languages, ii. to summarise reports and studies, iii. to help civil servants deliver briefing notes to the policymakers, iv. to analyse thousands of contributions to a public consultation questionnaire, v. to analyse and draw conclusions from open text feedback comments to policy documents published on the Have Your Say web portal.  All these AI-enabled tools are being tested for their reliability and correctness, which sometimes is doubtful, therefore any AI-enabled product should be checked and approved by humans prior of its sharing. 

Given that AI use seems to be an irreversible trend which, will, beyond any doubt, influence our way of life and work, this educational programme will bring insights into these questions and make the students familiarise themselves with all the concepts, the ‚Äòblack box‚Äô, the interactions between patterns of datasets as well as with the risks.  While it may be doubtful whether AI will eliminate job positions, it is for sure legitimate to believe that a worker trained in AI will definitely replace another worker who refuses to learn about AI.

As for my personal expectations after completing the course, I would like to seek a job in the EC‚Äôs AI Office, which has already been created following the implementation of the AI act in 2024. That way I will be able not only to effectively monitor and oversee the implementation of the AI act but also to suggest practical solutions, recommendations and tools for the upcoming challenges from the wider use of AI, the ultimate objective being that EU delivers more evidence-informed legislative proposals and policies.


**List of References**

Deloitte. (2018) The Fourth Industrial Revolution is here-are you ready?. Available from https://www2.deloitte.com/content/dam/Deloitte/tr/Documents/manufacturing/Industry4-0_Are-you-ready_Report.pdf [Accessed 19 July 2024].

European Commission. (2024) C(2024) 380 final Communication to the Commission Artificial Intelligence in the European Commission (AI@EC). Available from: 601a9e64-cdb9-4545-becb-1079ba8c457c_en (europa.eu) [Accessed 19 July 2024].

Hern A. & Milmo D. (February 24, 2023) Everything you wanted to know about AI ‚Äì but were afraid to ask. The Guardian. Available from: https://www.theguardian.com/technology/2023/feb/24/ai-artificial-intelligence-chatbots-to-deepfakes [accessed 20 July 2024].


## Understanding Artifical Intelligence

### In this module, I shall:

*Develop an understanding of the history and future of Artificial Intelligence. To include definition of AI, professional and ethical issues, as well as key application areas (Industry 4.0 and FinTech).

*Develop an understanding of AI fundamentals including the Alan Turing Test, knowledge representation and approaches to developing learning systems.

*Examine programming paradigms and algorithms for Machine Learning.

*Examine the preparation data for AI solutions.

*Develop programming abilities for learning algorithm implementation.

### On completion of this module, I will be able to:

*Understand the legal, ethical and professional issues brought up by AI and the impact of AI on society.

*Understand and critically analyse the essential concepts, principles, methods, techniques and problems of AI.

*Demonstrate a critical understanding of data requirements and programming paradigms applicable to AI.

*Apply and evaluate critically the various methods, tools and technologies applied to an AI project in order to develop an effective plan and delivery of solutions to a business problem.


### Collaborative Discussion 1

**Discussion Topic**

**Discuss why Artificial Intelligence is now ubiquitous and why it is important for companies to invest in Artificial Intelligence technologies.
Your discussion could consider/identify:  A typical company or industry that employs AI technologies, as well as the nature of the business to set the context for the discussion.
The economic benefits to the company or industry, and other implications of using Artificial Intelligence (AI) technology.  You should demonstrate that you understand the topic covered and ensure you use references to academic literature (journals, books, reports, etc.). This is activity will provide evidence of your personal growth.**

**Initial Post**

The customer recommendation system by Amazon, the world-leader retail online store, makes extensive use of AI-enabled tools to improve customer experience and satisfaction with a view to ultimately increasing the orders placed by users and the business revenues.  Amazon recommendations are based on the products purchased by the user in the past on the same site, their visits to the Amazon product web pages as well as to those purchased by other customers with similar interests.    

The company even makes a step further by combining related products and offering a new more upgraded order option to the user.  The product combination is accompanied with a new price tag, so customers are indirectly pushed to buy both items based on data of preferences from customers with similar interests.  Amazon recommendation system has been made possible thanks to the possibility of analysing 'big data' real-time, originating from the order history and preferences of Amazon customers around the world.  According to S. Yin and O. Kaynak (2015), 'big data refers to data sets whose size is beyond the ability of typical database software tools to capture, store, manage, and analyze', whereas the same authors argue that the customer-centric experience will primarily dominate the AI-enabled tools environment.

'The remarkable advances in computing power and the creation of the World Wide Web have facilitated the creation of very large data sets' (Russel, Stuart et al., 2021), which have subsequently contributed to the customisation of consumer order proposals.

To what extent do you believe that the protection of the personal data of customers is infringed when Amazon is collecting their preferences and making recommendations to them? To what extent do you believe that the use of AI-enabled tools for recommendations is nudging customers to buy more and more products from the same company, leading to disloyal competition practices?

**References**

Russell, Stuart, and Peter Norvig. Artificial Intelligence: a Modern Approach, Global Edition, Pearson Education, Limited, 2021. ProQuest Ebook Central.  Availble from http://ebookcentral.proquest.com/lib/universityofessex-ebooks/detail.action?docID=6563568 [Accessed on 1 August 2024]

S. Yin and O. Kaynak, "Big Data for Modern Industry: Challenges and Trends [Point of View]," in Proceedings of the IEEE, vol. 103, no. 2, pp. 143-146, Feb. 2015.  Available from https://ieeexplore.ieee.org/document/7067026 [Accessed on 2 August 2024]


**Summary**

The customer recommendation system by Amazon makes extensive use of AI-enabled tools to improve customer experience with a view to ultimately increasing orders placed by users and business revenues.  Amazon recommendations are based on the products purchased by the user in the past on the same site, their visits to the Amazon product web pages as well as to those purchased by other customers with similar interests.    

The company even makes a step further by offering a combination of products, accompanied with a new price tag, so customers are indirectly pushed to buy more items.  Amazon recommendation system has been made possible thanks to the possibility of analysing 'big data' real-time, originating from the order history and preferences of Amazon customers around the world.  This is line with what S. Yin and O. Kaynak (2015) have argued, namely that the customer-centric experience will primarily dominate the AI environment.

Peers reacting to the post have expressed their concern primarily when it comes to the question of clarity/transparency of how customer data are being used and also to the frustration that such recommendation system may generate if perceived by online customers as too intrusive and (Moore et al., 2015) ‚Äòcreepy‚Äô. 

It turns out that these companies have much more leverage than their customers concerning data collection and AI utilization.  This is why it is crucial that a legal/regulatory framework be adopted by governments. The EU AI Act (2024), one of the first attempts of a world regulator to put some boundaries and limits to the uncontrolled use of AI, seems to strike the right balance between innovation and safety/ethics. It allows researchers to experiment with AI tools in lab conditions, while at the same time prohibiting AI use representing an unacceptable risk.   It seems plausible that apart from the EU, other big regulators will follow suit and adopt similar measures, if they wish to continue trading with the EU Single market (the so called 'Brussels effect').

**References**

European Commission (2023) EU Artificial Intelligence Act, Available at https://artificialintelligenceact.eu/the-act/

Moore, R.S., Moore, M.L. and Shanahan, K.J., 2015. Creepy marketing: Three dimensions of perceived excessive online privacy violation. Marketing, pp.1-10.

S. Yin and O. Kaynak, "Big Data for Modern Industry: Challenges and Trends [Point of View]," in Proceedings of the IEEE, vol. 103, no. 2, pp. 143-146, Feb. 2015.  Available from https://ieeexplore.ieee.org/document/7067026 [Accessed on 2 August 2024]


### Collaborative Discussion 2

**Discussion Topic**

**Identify and discuss two machine learning algorithms and the context in which they can be employed. Your discussion could consider:  Supervised and/or unsupervised learning algorithms. For example, if considering supervised learning, what type of learning algorithms would be ideal for the solution?  The strengths and weaknesses of this approach to learning.
You should demonstrate that you understand the topic covered and ensure you use references to academic literature (journals, books, reports, etc.). This is activity will provide evidence of your personal growth.**

**Initial Post**

‚ÄòA decision tree is a [‚Ä¶] supervised learning algorithm, which is utilised for both classification and regression tasks‚Äô (IBM). It consists of a root, internal nodes and leaves.  ‚ÄòThe internal nodes of the tree represent a test on an attribute or subset of attributes‚Äô (Cohen, 2021).  Decision trees (DTs) measure ‚Äòthe difference of entropy before and after a split‚Äô (information gain).  ‚ÄòThe node with the highest gain becomes the root node‚Äô (Bell, 2020).  DTs can easily classify data and predict the outcome based on the training data, so they are widely used, among others, for data mining, in bank/finance services to check the trustworthiness of clients, in customer service, where they direct phones calls/emails to the appropriate department (sales, after-sales services, repair/feedback, payment methods)

DTs are simple to understand and interpret as they can be visualised, can process both numerical and categorical data, support a certain degree of explainability contratry to the ‚Äòblack box‚Äô of artificial neural network and they require little to no data preparation (Learn Scikit).  Nonetheless, DTs can become overly complex as they cannot generalise well to new data (overfitting) and even slight variations can deliver a different outcome (IBM).

A support vector machine (SVM) is another supervised learning approach for classifying objects/images.  ‚ÄòClassification problems aim to predict the category (class) to which a given input belongs‚Äô.  SVM algorithm identifies the optimal hyperplane among an infinite set because it assesses some data more critical than others for finding the best diving line/hyperplane: the support vectors (Prinzi et al., 2024).  SVM are used in the analysis of radiological images (breast cancer detection, cardiac disease diagnosis etc) as well as for identifying fraudulent credit card activity, recognising handwritten digits and letters etc

A very good feature of SVM is that only a small training set is needed to provide very good results (Tzotsos et al., 2008), so they can generalise quite well to new data (avoiding overfitting). SVM are also effective in high-dimensional spaces and efficient in memory usage as they only need to store a small fraction of the training data. However, SVM can be also computationally intensive.  The number of possible kernels/transformations is infinite and can make it hard to choose the right one. 

**References**

Cohen S. MD (2021) The basics of machine learning: strategies and techniques in Artificial Intelligence and Deep Learning in Pathology. Available from https://www.sciencedirect.com/topics/computer-science/decision-tree-algorithm [Accessed on 3 September 2024]

Bell, J. (2020) Machine Learning: Hands-On for Developers and Technical Professionals. 2nd ed. Chichester: Wiley

European Information Technologies Certification Academy (7 August, 2023) What are some advantages of using support vector machines (SVMs) in machine learning applications? Available from https://eitca.org/artificial-intelligence/eitc-ai-mlp-machine-learning-with-python/support-vector-machine/support-vector-machine-introduction-and-application/examination-review-support-vector-machine-introduction-and-application/what-are-some-advantages-of-using-support-vector-machines-svms-in-machine-learning-applications/ [Accessed on 3 September]

IBM, What is a decision tree? Available from https://www.ibm.com/topics/decision-trees [Accessed on 4 September 2024]

Learn Scikit Available from https://scikit-learn.org/stable/modules/tree.html [Accessed on 4 September 2024]

Prinzi, F., Currieri, T., Gaglio, S. et al. (2024) Shallow and deep learning classifiers in medical image analysis. Eur Radiol Exp 8, 26. https://doi.org/10.1186/s41747-024-00428-2

Tzotsos, Angelos & Argialas, Demetre (2008) Support Vector Machine Classification for Object-Based Image Analysis.  Available from https://www.researchgate.net/publication/225929583_Support_Vector_Machine_Classification_for_Object-Based_Image_Analysis [Accessed on 4 September]


**Summary**

‚ÄòA decision tree, a [‚Ä¶] supervised learning algorithm, is utilised for both classification and regression tasks‚Äô (IBM, ND). Decision trees (DTs) measure ‚Äòthe difference of entropy before and after a split‚Äô (information gain).  ‚ÄòThe node with the highest gain becomes the root node‚Äô (Bell, 2020).  DTs can easily classify data and predict the outcome based on the training data, so they are widely used for data mining, in banking/finance, customer service.  They are simple to understand as they can be visualised, can process both numerical and categorical data, support a certain degree of explainability contrary to the ‚Äòblack box‚Äô of artificial neural network and they require little to no data preparation (Learn Scikit, ND).  

Nonetheless, DTs can become overly complex as they work well with training data but not when applying new or unseen real-world data (overfitting) (Wang et al., 2010).  One way to avoid this is by pre-pruning and post-pruning, which help reduce overfitting (Bramer, 2007) and improve the accuracy of the overall classification, when applied to the validation dataset (Song, 2015).

Support vector machines (SVMs), another supervised learning approach for classifying objects/images, identify the optimal hyperplane among an infinite set because they assess some data more critical than others for finding the best diving line/hyperplane: the support vectors (Prinzi et al., 2024).  SVMs are highly effective in handling high-dimensional data, which is invaluable in fields like text classification and bioinformatics (Ben-Hur & Weston, 2010), classification of complex patterns in medical images (Litjens et al., 2017).  A very good feature of SVMs is that only a small training set is needed to provide very good results (Tzotsos et al., 2008), so they can generalise quite well to new data.  

However, SVMs can be also computationally intensive, sensitive to parameter tuning, they have difficulty in interpreting complex models and also, they can face scalability issues when applied to extremely large data sets.  Training an SVM on millions of samples can become impractical due to memory and computational constraints (Tabsharani F.)

**Reference list**

Bell, J. (2020) Machine Learning: Hands-On for Developers and Technical Professionals. 2nd ed. Chichester: Wiley

Ben-Hur, A. & Weston, J. (2010). A user‚Äôs guide to support vector machines. Methods in Molecular Biology, 609, 223-239.

Bramer, M., (2007). Avoiding overfitting of decision trees. Principles of data mining, pp.119-134.

IBM, (ND) What is a decision tree? Available from https://www.ibm.com/topics/decision-trees [Accessed on 4 September 2024]

Learn Scikit, (ND) Available from https://scikit-learn.org/stable/modules/tree.html [Accessed on 4 September 2024]

Litjens, G. et al. (2017). A survey on deep learning in medical image analysis. Medical Image Analysis, 42, 60-88.

Prinzi, F., Currieri, T., Gaglio, S. et al. (2024) Shallow and deep learning classifiers in medical image analysis. Eur Radiol Exp 8, 26. https://doi.org/10.1186/s41747-024-00428-2

Song YY, Lu Y. (25 April 2015), Decision tree methods: applications for classification and prediction. Shanghai Arch Psychiatry. 27(2):130-5. DOI: 10.11919/j.issn.1002-0829.215044

Tabsharani F. (ND), Support vector machine (SVM). Available from https://www.techtarget.com/whatis/definition/support-vector-machine-SVM#:~:text=A%20support%20vector%20machine%20(SVM)%20is%20a%20type%20of%20supervised,data%20set%20into%20two%20groups [Accessed on 9 September]

Tzotsos, Angelos & Argialas, Demetre (2008) Support Vector Machine Classification for Object-Based Image Analysis.  Available from https://www.researchgate.net/publication/225929583_Support_Vector_Machine_Classification_for_Object-Based_Image_Analysis [Accessed on 4 September]

Wang, T., Qin, Z., Jin, Z. and Zhang, S., 2010. Handling over-fitting in test cost-sensitive decision tree learning by feature selection, smoothing and pruning. Journal of Systems and Software, 83(7), pp.1137-1147.


### Individual Essay: Artificial Intelligence and its Applications

**Assignment Details**

**A local start-up finance company is vaguely aware of AI and the potential benefits, as well as the harms that these technologies can cause to society. This has caused the Senior Management of the company to have some reservations about AI. As an Artificial Intelligence consultant, you are to present a report to persuade the management of the company to accept your idea to employ AI‚ÄØtechnologies‚ÄØto improve business processes for competitiveness.  Your report should identify three key areas to which the company could apply AI technologies to facilitate its operations and increase their return on investment. The report should also consider data needed, as well as the sort of approach that will be required to develop an AI system.**

**How to tap into the AI potential and further boost our competitiveness** 

As a finance company, we grant loans to individuals and businesses (personal and corporate branch), including merchants, retailers and manufacturers for the purchase of goods and services with securities (Britannica, ND).  To further enhance our competitiveness prospects and maintain our market share in a challenging and unpredictable financial framework, there is a pressing need our company endorse a series of Artificial Intelligence (AI) enabled tools. These AI-enabled tools will aim to i) facilitate current internal business/customer operations, ii) automatically process credit risk of customers and iii) target creditworthy clients by personalising investment opportunities based on their profile.  
This report explores the phasing-in of some AI-enabled tools in our company, the expected benefits (turnover increase and market position improvement) as well as the pitfalls and potential risks we have to take into account.   

My department has made use of the NAFTA framework before making the business case for staged AI adoption.  First, we identified the problem (Need), explored to what extent the AI-solution is aligned with the company‚Äôs broader strategy (Alignment), and determined its financial viability (Finance) (off-the-shelf, open source and bespoke), and thus guarantee return on investment.   Subsequently, we duly tested the need for AI implementation and how to best integrate it in the corporate operations (Test) and then we identified the tools, which will enable us to assess the risks involved in the technology (Analyse) (Mcilrath, B. & Kotnour. T, 2002; Terence Tse et al., 2021; Poree, J., 2022; University of Oxford, 2023).

i) The first key area of AI deployment could be the customer service.  We propose the extensive use of automated voice assistants answering customers‚Äô simple questions and directing the complex ones to the appropriate department as well as the introduction of chatbots (computer programs that use artificial intelligence to imitate a conversation with a human-Bowman, 2024) for customer web/mobile application services.  Both tools will leverage natural language processing (NLP) i.e. speech and speech-to-text recognition, using supervised learning algorithms in particular Decision Trees (DTs)- (the technological characteristics of the envisaged algorithmic models and the trade-offs will be discussed further below).  By answering questions and completing routine tasks 24/7 (IBM, ND), the AI-solution will enable us to reorientate human resources to more strategic-wise departments as the customers‚Äô questions will be automatically answered, whereas the more complex ones will be directed to the responsible department (follow-up on pending loan applications, complaints etc).  That way, we will optimise the customer experience and make sure we deal with the most critical ones.   Nonetheless, some customers may feel frustrated by this automated way of dealing with inquiries as the AI-solution may not be able to understand the question upfront and it may direct the phone call to the wrong department, which may result in us losing potential good customers. Or else, the chatbots‚Äô answers may be the result of hallucination (generating false information and presenting it in a convincing way).

ii) The second area of AI deployment could be credit risk analysis, scoring and loan approval.  In this case also, supervised learning algorithms (DTs) can also be used.  They will automatically evaluate the creditworthiness of potential customers based on their economic behaviour, financial data, credit history, potential cases of default using data extracted from centralised databases of the intra-banking system as well as on the income data from tax authorities, following the explicit consent of the candidate customers (individual/corporate).  Such algorithms will thus help our company with risk assessment, credit scoring and document verification and ultimately, they will help us speeding up the loan approval procedures.  However, the financing approval will be only complemented by automated AI tools, as human intervention will be necessary for the final decision (Bowman, 2024; PEX, 2024; Deloitte, ND; IBM, ND).  Given that ‚ÄòAI algorithms can analyse vast amounts of data to identify patterns and assess creditworthiness more accurately, [‚Ä¶] this can lead to fewer loan defaults, reduced risk provisions, and improved profit margins, [‚Ä¶] [This can] improve risk management leading to substantial cost savings through improved fraud detection and creditworthiness assessments‚Äô (Chlouverakis, 2024).  
On the other hand, such processing of sensitive financial data may result in ‚Äòfalse positives‚Äô and ‚Äòfalse negatives‚Äô due to algorithmic bias, data misuse, data privacy and security infringements.  For example, the algorithm may reject people from a given social, ethnic, social background (bias), offer a higher interest rate for the loan applicant or ask for a higher value security in comparison to other potential customers of the same creditworthiness level.

iii) The third area of AI deployment could be the provision of proactive personalised financing services to customers by means of recommendations (investment advice/financing offers), based on customer journeys, peer interactions, risk preferences, and financial goals (Cloud.google, ND) as well as ‚Äòcustomers' transaction history [and] spending habits [‚Ä¶]. Through such personalisation, institutions can improve customer satisfaction and loyalty‚Äô (Onestream, ND; IBM, ND).  In this case, an envisaged AI-solution could be an unsupervised learning algorithm (K-means), which would cluster potential customers with similar financial goals and income levels based on unlabelled data.  That way, we could be able to make personalised financial services recommendations in a proactive way, increasing our earnings and market share.  On the downside, such solution may pose some explainability challenges, as its inherent complexity and opacity complicate the understanding of their decision-making processes (Chlouverakis, 2024).  In addition, serious data privacy and safety issues may equally be raised with similar AI algorithms.
As mentioned above, to facilitate current internal business and customer operations, and automatically analyse the credit risk of loan applicants (individual/corporate), we propose to make use of the Decision Tree (DT) algorithmic model.   A decision tree is a supervised machine learning model, which makes use of labelled input and datasets for output to train a model.   It consists of a root, internal nodes and leaves.  ‚ÄòThe internal nodes of the tree represent a test on an attribute or subset of attributes‚Äô (Cohen, 2021).   DTs can easily classify data and predict the outcome based on the training data, so they are widely used for data mining, in bank/finance services to check the trustworthiness of clients, in customer service, where they direct phones calls/emails to the appropriate department (sales, after-sales services, repair/feedback, payment methods).  Therefore, this model is suitable for us to deploy in the customer service department and in the credit risk analysis department.

It has to be borne in mind, though, that all techniques and algorithms have their own particular strengths and weaknesses.   On the one hand, DTs are simple to understand and interpret as they can be visualised, can process both numerical and categorical data, support a certain degree of explainability contrary to the ‚Äòblack box‚Äô of artificial neural network and they require little to no data preparation (Learn Scikit, ND).   

On the other hand, DTs can become overly complex as they cannot generalise well to new data (overfitting) and even slight variations can deliver a different outcome (IBM, ND).   This is the reason why DTs are considered as unstable, in the sense that once an attribute value is modified, the result may be also modified, giving another outcome.  In our company, this may lead to ‚Äòfalse negatives‚Äô or ‚Äòfalse positives‚Äô.  The repercussions could be: not convincingly answering the clients‚Äô requests on the phone/web page (chatbox), forwarding a complaint to the wrong department, failing to identify the customer, which may lead to their frustration.  As for the credit risk analysis, the DT may give the wrong outcome as for the creditworthiness of a loan applicant.  Nevertheless, given that the final decision of the loan application will not entirely rely on the algorithm, but it will need human intervention, the repercussions of this disadvantage are deemed to be very limited.  

As for the difficulty of this model to generalise well to new data, two of the methods to address this disadvantage are pre-pruning (preventing the generation of non-significant branches) or post-pruning (removing branches after generating a full decision tree) (Song, 2015).

Regarding the targeting of creditworthy clients by personalising investment opportunities based on their profile, we envisage using the most common unsupervised learning model, which is clustering (detecting potentially useful clusters of input examples) and particularly the centroid model (k-means).  According to this model, ‚Äò[e]ach one of the clusters has a centroid [‚Ä¶], a point where the distance of the objects will be calculated.  The clusters are defined by an iterative process on the distances of the objects to calculate which are nearest to the centroid.  In unsupervised learning, the agent learns patterns in the input without any explicit feedback‚Äô (Russel et al., 2021).

Retail industry is heavily investing in analysing customer preferences, needs and buying behaviour through clustering techniques.   Such technique is also used in a lot of fields such as statistics, medicine, patterns identification etc (Suyal, 2024).  ‚ÄòSocial media network analysis uses clustering to determine communities of users.  With so many users on Facebook, [‚Ä¶], using these sorts of techniques can refine advertising so that certain ads go to specific groups of customers‚Äô (Bell, 2020).   In addition, website logs and search results are often clustered to show more relevant search result groups, whereas clustering is also used to refine search engine queries (Bell, 2020).

Through this model, we can group customers with similar income/creditworthiness ratings into clusters and proactively offer them personalised investment and financing advice/recommendations.  Potential mismatches of clusters and customers could be alleviated, as in that case also human intervention will be determining.

Following the aforementioned analysis, our finance company could start deploying the envisaged AI-solutions as a pilot.  Then, we will collect experience/data from the customers and staff alike, proceed to the assessment of the economic results achieved and then potentially deploy the AI-solutions at a larger scale.  Some subsequent areas of potential AI deployment could be fraud detection and prevention, cash flow forecasting, automated reporting, regulatory compliance, investment management, invoice processing and predictive analytics.

**Reference list**

Britannica, (ND) Available from  https://www.britannica.com/money/finance-company [Accessed on 20 September 2024]

Bell, J. (2020) Machine Learning: Hands-On for Developers and Technical Professionals. 2nd ed. Chichester: Wiley. Available from https://learning.oreilly.com/library/view/machine-learning-2nd/9781119642145/?sso_link=yes&sso_link_from=university-of-essex [Accessed on 15 September 2024]

Bowman J. (20 August, 2024) How Artificial Intelligence is Used in Finance Learn how AI is transforming the financial sector Available from https://www.fool.com/investing/stock-market/market-sectors/information-technology/ai-stocks/ai-in-finance/#:~:text=AI%20is%20being%20used%20in,insurance%2C%20and%20even%20customer%20service [Accessed on 15 September 2024]

Chlouverakis K. (26 April, 2024) How artificial intelligence is reshaping the financial services industry Available from https://www.ey.com/en_gr/financial-services/how-artificial-intelligence-is-reshaping-the-financial-services-industry [Accessed on 10 September 2024]

Cloud. Google (ND), Available from https://cloud.google.com/discover/finance-ai 

Cohen S. MD (2021) Chapter 2: The basics of machine learning: strategies and techniques, Artificial Intelligence and Deep Learning in Pathology. Available from 
https://www.sciencedirect.com/topics/computer-science/decision-tree-algorithm [Accessed on 3 September 2024]

Crumley B., (20 June, 2024), Finance Jobs Will Be Changed the Most by AI, Report Says, Available from https://www.inc.com/bruce-crumley/finance-jobs-will-be-changed-most-by-ai-report-says.html [Accessed on 19 September 2024]

Deloitte (ND), How Artificial Intelligence is Transforming the Financial Services Industry Available from https://www.deloitte.com/ng/en/services/risk-advisory/services/how-artificial-intelligence-is-transforming-the-financial-services-industry.html [Accessed on 18 September 2024]

Duboue, P. (2020) The Art of Feature Engineering: Essentials for Machine Learning. Cambridge: Cambridge University Press

Finio M. & Downie A. (8 December, 2023), IBM What is AI in finance? Available from https://www.ibm.com/topics/artificial-intelligence-finance [Accessed on 15 September 2024] 
IBM, (ND) Available from https://www.ibm.com/topics/decision-trees [Accessed on 4 September 2024]	

McIlrath, B.J., & Kotnour, T. (2002). Process Alignment for Strategic Implementation. Industrial Engineering and Management Systems, University of Central Florida Available from https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=010f80bf420b7d623cf987e5727a0f0574867620 [Accessed on 19 September 2024]

Learn Scikit (ND), Available from https://scikit-learn.org/stable/modules/tree.html [Accessed on 4 September 2024]

Oti, E. et al (2021). Comprehensive Review of K-Means Clustering Algorithms. International Journal of Advances in Scientific Research and Engineering. Available from https://www.researchgate.net/publication/354547481_Comprehensive_Review_of_K-Means_Clustering_Algorithms [Accessed on 9 September 2024]

Nishan, J. (26 May 2023) Mastering data clustering: Your comprehensive guide to K-means and K-means++, AI Accelerator Institute. Available from https://www.aiacceleratorinstitute.com/mastering-data-clustering-your-comprehensive-guide-to-k-means-and-k-means/ [Accessed on 9 September 2024)

PEX Network (11 July, 2024) How AI is transforming financial services: Key roles and functions Available from https://www.processexcellencenetwork.com/ai/articles/ai-transforming-financial-services [Accessed on 15 September]

Poree J. (23 September 2022), Nexus FrontierTex How Do I Identify Valid Use Cases for AI Within My Enterprise? Available from hhttps://nexusfrontier.tech/how-do-i-identify-valid-use-cases-for-ai-within-my-enterprise/ [Accessed on 12 September 2024]

Russell, S, & Norvig, (2021), Artificial Intelligence: a Modern Approach, Global Edition, Pearson Education, Limited, Harlow. Available from: ProQuest Ebook Central. [Accessed on 19 September 2024].

Suyal, M. & Sharma, S. (2024) A Review on Analysis of K-Means Clustering Machine Learning Algorithm based on Unsupervised Learning. Journal of Artificial Intelligence and Systems. Available from https://www.researchgate.net/publication/379878557_A_Review_on_Analysis_of_K-Means_Clustering_Machine_Learning_Algorithm_based_on_Unsupervised_Learning [Accessed on 8 September]

Song YY, Lu Y. (25 April 2015), Decision tree methods: applications for classification and prediction. Shanghai Arch Psychiatry. 27(2):130-5. DOI: 10.11919/j.issn.1002-0829.215044
Terence Tse et al.  (29 April, 2021) Five steps for companies to make AI pilots a success Available from https://blogs.lse.ac.uk/businessreview/2021/04/29/five-steps-for-companies-to-make-ai-pilots-a-success/ [Accessed on 14 September 2024]

University of Oxford (2023), Artificial Intelligence Programme, Module 6, Unit 2 Infographic Transcript 

Anon (ND), Revolutionizing Finance: The AI in Finance Advantage, Available from https://www.onestream.com/blog/ai-in-finance/ [Accessed on 19 September 2024]


### Artificial Intelligence (AI) Solution Implementation

**Assignment Details**

**You are required to carry out an experiment using‚ÄØthe‚ÄØWEKA software tool and the appropriate dataset available at UCI or Kaggle. You will need to demonstrate to‚ÄØthe‚ÄØsenior management of the start-up company discussed in Unit 9, the feasibility of the AI technology deployment in at least one of the key areas you identified in your Unit 9 report. It does not have to be an exact solution, but you must demonstrate the application of the idea and how the approach and methods of the experiment can be transferred.**

**Implementation report**

I. Business context

Following the previous report of the IT department concerning the need for our start-up finance company to embrace Artificial Intelligence (AI) enabled tools in its operations, with this implementation report, we would like to demonstrate the feasibility of AI deployment, which will aim to enhance our competitiveness and improve our market share.  
As previously discussed, there is a pressing need for our company to endorse a series of AI-enabled tools concerning three key areas: i) day-to-day business/customer operations (forwarding phone calls/emails to the competent department, smooth handling of complaints) ii) automatic credit risk analysis, scoring and loan approval and finally iii) proactive targeting of customers based on their profile through personalised recommendations for financial products/services.  To better illustrate the case for the feasibility of the AI-deployment, my department has explored how we could deploy AI-solutions in one of the aforementioned key areas, namely the credit risk analysis for loan applicants.   As already stressed, for this key area we propose making use of supervised learning methods and, in particular, of Decision Tree (DT) algorithms.   Our concept project has benefited from the CRISP-DM process model (Cross-Industry Standard Process for Data Mining) and its process model descriptions: a. business understanding, b. data understanding, c. data preparation, d. modelling, e. evaluation and f. deployment (Schr√∂er et al., 2021).

II. Justification of the dataset choice 

To analyse the credit risk/scoring of loan applicants and find out what type of data is required for the final decision to grant/reject a loan application, we have primarily based our proposal on the domain knowledge of colleagues working in the department responsible for processing loan applications.  The datasets, which are valuable for the ‚Äòmanual‚Äô processing and could be used to feed in the algorithmic model, in order to predict the credit risk of loan applicants, are the following: corporate/individual loan applicant status, gender, marital status, education, turnover for companies/yearly income for individuals, property value, default cases in the past, cases of delay in paying back instalments, level of securities, the purpose of the loan (service/investment/goods), the level of yearly tax returns, the level of existing private debts or of debts to the state authorities as well as pending mortgage/credit card debts.  

Some of this data could be filled in on our company software application by the loan applicants themselves, whereas the most crucial ones, following their consent, could be retrieved, via interoperability methods, from the intra-banking system and the tax authorities‚Äô databases.  We cannot now argue that all the above datasets will be required for our envisaged model, however, all dataset categories add information in the loan application procedure.  After having applied the model and evaluated its results, we will be better placed to decide which of the data will be absolutely required.

The above process of selecting the appropriate datasets constitutes a part of feature engineering, ‚Äòwhich include feature selection and extraction to reduce data dimensionality and eliminate redundancy and noise‚Äô (Wang et al., 2021).  Choosing the appropriate subset of attributes means optimally reducing the volume of the existing data, according to the defined criteria set already by the responsible department (knowledge-based selection).  The primary purpose of such selection is to enhance the accuracy of the model prediction by discarding characteristics with little impact and by retaining at the same time the characteristics having an added value (Wang et al., 2021).  


III. Justification of the approach to developing the prediction model

Given that the factors of creditworthiness are already known by the company experts, we proposed for this AI-solution the use of the supervised learning method.  The model, which will be fed in by a number of attribute values with an information gain, is expected to result to two main prediction outcomes: 

i)	cases of loan applicants with a high level of creditworthiness, where the model will advise in favour of the loan approval (Class: Yes) and 

ii)	cases of loan applicants with a dubious creditworthiness level, where the model will advise against granting a loan (Class: No).  

There might also be cases somewhere in the middle of these extremes, where loan applicants may have experienced some minor financial problems in the past, which do not make them qualify to the creditworthy customers, so further research/cross-checking should follow before the final decision is made (loan approval/rejection).   

Nevertheless, according to our project, the envisaged AI-solution will only augment human capacity, because human intervention will continue being necessary for loan approval or rejection (Bowman, 2024; PEX, 2024; Deloitte, ND; IBM, ND).  As explained in our initial report, given that ‚ÄòAI algorithms can analyse vast amounts of data to identify patterns and assess creditworthiness more accurately, [‚Ä¶] this can lead to fewer loan defaults, reduced risk provisions, and improved profit margins, [‚Ä¶] (Chlouverakis, 2024).  

Our AI-model will be further tested/evaluated so that ‚Äòfalse positives/FP‚Äô and ‚Äòfalse negatives/FN‚Äô, due to algorithmic bias, data misuse, data privacy and security infringements, are avoided to the extent possible.  To give you an example of algorithmic bias, people from a given social, ethnic, social background may be biased, offered a higher interest rate for the loan applicant or asked for a higher value security in comparison to other potential customers of the same creditworthiness level (Archontas, 2024).  To improve our competitiveness and market share, we must avoid cases of FP as much as possible, while the existence of FN is less problematic, as these instances will be further examined by the responsible department (Specificity/accuracy indicator). Granting loans to FPs will jeopardise our survival and will have an adverse effect on the company assets‚Äô value.

IV. Rationale for the machine learning algorithms used

We propose to make use of the Decision Tree (DT) algorithmic model, which constitutes a straightforward, easy-to-see and interpret algorithm of supervised learning with a high degree of explicability and replicability.  A DT uses labelled input and datasets for output to train a model and consists of a root, internal nodes and leaves.  ‚ÄòThe internal nodes of the tree represent a test on an attribute or subset of attributes‚Äô (Cohen, 2021).   DTs can easily classify data and predict the outcome based on the training data; this is why the model is suitable for us to deploy for the credit risk analysis/scoring/loan approval.

In the initial report, we pointed out that DTs are simple to understand as they can be visualised, can process numerical/categorical data, may support a certain degree of explainability contrary to the ‚Äòblack box‚Äô of artificial neural network and they require little to no data preparation (Learn Scikit, ND).   While DTs for credit risk analysis may be effective and efficient, we have to take into account that DTs can become overly complex as they cannot generalise well to new data (overfitting) and even slight variations can deliver a different outcome (IBM, ND).   This is the reason why DTs are considered as unstable, in the sense that once an attribute value is modified, the outcome may be also modified.  
To illustrate our AI-solution proposal, we used open data with similar characteristics to our business available on Kaggle.  More concretely, we used the data file Credit_risk_customers (file in .csv format https://www.kaggle.com/datasets/ppb00x/credit-risk-customers )

This dataset consists of 20 features of 1 000 customers/instances and could be used to predict if the loan applicant could be given credit (good/bad credit risk).  The 20 features/attributes of the file are the following: checking status, duration, credit history, purpose, credit amount, savings status, employment, instalment commitment, personal status, other parties, residence since, property magnitude, age, other payment plans, housing, existing credits, job, number of dependents, own telephone, foreign worker, class.

For the sake of our pilot project run in a testing environment, we selected 9 features, which are significant from the domain knowledge point of view and subsequently removed/discarded the remaining ones, as we considered that they do not provide much added value.  After discarding the redundant features, we kept the initial 1 000 instances with 10 features (9 attributes and 1 class): checking status, duration, credit history, credit amount, savings status, employment, other payment plans, housing, existing credits, class/outcome.

V. Analysis of the outputs

For our analysis, we used the WEKA tool and deployed the Classification functionality, the Decision Tree and the J48 algorithmic model (Annex).  Significant tests were carried out to examine the performance differences between the prediction models.  We performed six (J48) experiments and tested their validity and accuracy with different WEKA parameters: 

i)	unpruned vs. pruned tree, 

ii)	cross-validation 10-20, 

iii)	minNumObj 2-10 

iv)	tree visualisation 

(screenshots of the six model experiments can be found in the Annex)

A.	unpruned tree, cross-validation 10, minNumObj2

B.	unpruned tree, cross-validation 20, minNumObj2

C.	pruned tree, cross-validation 10, minNumObj2

D.	pruned tree, cross-validation 20, minNumObj2

E.	pruned tree, cross-validation 10, minNumObj 10

F.	pruned tree, cross-validation 20, minNumObj 10

Our assumption is that DTs should have limited number of leaves, so the visualisation is facilitated.  If this is not the case due to multiple features, we tried tree pruning.  

1. The model of unpruned tree with cross validation 10 and minNumObj2 produced an unpruned tree with 218 leaves (313 size of tree), with a percentage of 70.4% correctly classified instances vs. 29.6% of incorrectly classified instances.  The tree visualisation seems unmanageable and not clear enough.
2.The model of unpruned tree with cross validation 20 and minNumObj 2, produced an unpruned tree with 218 leaves (313 size of tree) and classified correctly 70.5% of the instances vs. 29.5% of incorrectly classified instances.  The tree visualisation seems again unmanageable and not clear enough.
3. The model of pruned tree with cross-validation 10 and minNumObj 2 produced a pruned tree with 60 leaves (92 size of tree) and 72.8% correctly classified instances vs. 27.2% incorrectly classified instances. Tree visualisation is improved, but it remains unclear.
4. The model of pruned tree with cross-validation 20 and minNumObj 2 produced a pruned tree with 60 leaves (92 size of tree), and 74.2% correctly classified instances vs. 25.8% incorrectly classified instances.  Although accuracy reaches the highest level out of the six experiments, nevertheless, the tree visualisation is not straightforward and does not help.
5. The model of pruned tree with cross-validation 10 and minNumObj 10 produced a pruned tree with 17 leaves (24 size of tree) and 73.4% of correctly classified instances vs. 26.6% of incorrectly classified instances.  The tree visualisation seems improved and can be easily consulted.
6. The model of pruned tree with cross-validation 20 and minNumObj 10, produced a pruned tree with 17 leaves (24 size of tree) and 74% of correctly classified instances vs. 26% incorrectly classified instances.  The tree visualisation is straightforward, and the accuracy level is quite high (74%), although not as high as the model experiment 4 (74.2%).
However, given the simplicity of the tree and the particularly high level of accuracy in detecting correctly classified instances, we suggest adopting this model algorithm.  

Due to the imbalance of the instances, we applied SMOTE (Synthetic Minority Oversampling Technique)/Randomise filters (see the results in the Annex, pp. 22-25).

VI. Demonstration that the application of the approach/methods can be applied to the identified problem 

As Foody (2023) pointed out, it is of utmost importance for the classification quality to what extent the model predictions are accurate.  The term ‚Äòclassification accuracy‚Äô denotes the amount of error included in the dataset and shows if the classification model is effective for a particular case.  ‚ÄòThe error can be calculated by comparing the classifier‚Äôs labels with reality.  In practice, the labels predicted by the classifier are compared against those obtained from a reference standard‚Äô. 
If the accuracy does not reach a sufficient level, this might constitute the rationale for further fine-tuning the parameters and enhancing the classifier model.  If the accuracy level remains low, we are obliged to discard the classifier model and choose another one which performs better (Foody, 2023).  This is why we tested six models and found out, following fine-tuning of the parameters of the J48 model (WEKA tool), that the best prediction outcomes and tree visualisation are produced by the model 6.  

![image](https://github.com/user-attachments/assets/637ecb45-7921-4b01-b9f2-7555fca93eaa)

When making use of the CRISP-DM process model and in particular when performing the modelling, the evaluation and deployment phases of the envisaged AI-solution, we must avoid a high rate of False Positives (FP) to prevent our finance company from granting loans to non-creditworthy loan applicants.  On the other hand, it is in our company best interest to seek a high rate of True Negatives (TN), cases that were classed as negative and also have a negative label in the reference data.  The determining factor for choosing the appropriate model algorithm should be ‚Äòinverse recall‚Äô/‚Äòspecificity‚Äô.  This term defines the proportion of real negative cases that are correctly predicted negative (True Negative Rate/tnr) (Powers, 2007).

![image](https://github.com/user-attachments/assets/21956226-9ad8-437d-953e-873444e257ca)

Having said the above, a bigger sample size will be required to further update and evaluate the suggested model, given that, for experiment purposes, we used datasets from the Kaggle repository and not our own.  We believe that with further development and validation, our prediction modelling algorithm will enable us to successfully analyse the credit risk of loan applicants and therefore to support and speed up the loan approval process.
‚ÄÉ

**References**

Archontas N. (2024), Artificial Intelligence and its applications, Individual Essay, Module 1, Understanding Artificial Intelligence, UoEO

Bowman J. (20 August, 2024) How Artificial Intelligence is Used in Finance Learn how AI is transforming the financial sector Available from https://www.fool.com/investing/stock-market/market-sectors/information-technology/ai-stocks/ai-in-finance/#:~:text=AI%20is%20being%20used%20in,insurance%2C%20and%20even%20customer%20service [Accessed on 15 September 2024]

Chlouverakis K. (26 April, 2024) How artificial intelligence is reshaping the financial services industry Available from https://www.ey.com/en_gr/financial-services/how-artificial-intelligence-is-reshaping-the-financial-services-industry [Accessed on 10 September 2024]

Cohen S. MD (2021) Chapter 2: The basics of machine learning: strategies and techniques, Artificial Intelligence and Deep Learning in Pathology. Available from https://www.sciencedirect.com/topics/computer-science/decision-tree-algorithm [Accessed on 3 September 2024]

Deloitte (ND), How Artificial Intelligence is Transforming the Financial Services Industry Available from https://www.deloitte.com/ng/en/services/risk-advisory/services/how-artificial-intelligence-is-transforming-the-financial-services-industry.html [Accessed on 18 September 2024]

Foody, G. M. & Huang, S. (2023) Challenges in the real world use of classification accuracy metrics: From recall and precision to the Matthews correlation coefficient. PloS one 18 (10): e0291908‚Äìe0291908. Available from https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0291908 [Accessed on 5 October 2024]

IBM, (ND) Available from https://www.ibm.com/topics/decision-trees [Accessed on 4 September 2024]	

Learn Scikit (ND), Available from https://scikit-learn.org/stable/modules/tree.html [Accessed on 4 September 2024]

PEX Network (11 July, 2024) How AI is transforming financial services: Key roles and functions Available from https://www.processexcellencenetwork.com/ai/articles/ai-transforming-financial-services [Accessed on 15 September]

Powers, D.M.W. (2007), Evaluation: from precision, recall and F-measure to ROC, Informedness, Markedness & Correlation, *AILab, School of Computer Science, Engineering and Mathematics, Flinders University, South Australia, Australia Available from https://www.researchgate.net/publication/228529307_Evaluation_From_Precision_Recall_and_F-Factor_to_ROC_Informedness_Markedness_Correlation [Accessed on 10 October 2024]

Schr√∂er, C., Kruse, F. & Gomez, J.M. (2021) A Systematic Literature Review on Applying CRISP-DM Process Model. Procedia computer science. 181526‚Äì534. Available from https://www.sciencedirect.com/science/article/pii/S1877050921002416?via%3Dihub [Accessed on 5 October 2024]

Wang, Z., Xia, L., Yuan, H., Srinivasan, R.S. & Song, X. (2022) Principles, research status, and prospects of feature engineering for data-driven building energy prediction: A comprehensive review. Journal of Building Engineering. 58105028.

WEKA (Waikato Environment for Knowledge Analysis) tool Available from https://www.waikato.ac.nz/int/research/institutes-centres-entities/institutes/artificial-intelligence-institute/research/software/


## Numerical Analysis

### In this module I shall:

*Develop a systematic understanding of foundational mathematical principles and methods, as well as core and specialised concepts underpinning computing logic.

*Understand the foundation for the development and application of programming and data-driven techniques, from both a theoretical and practical viewpoint.

*Facilitate the ability to interpret the results generated when using these data science and ai tools.

*Gain an understanding of the real-world applications of these computational tools, and contemporary issues related to these computational techniques.

*The opportunity to take a reflective and independent approach to the learning process.

### On completion of this module I will be able to:

*Demonstrate systematic understanding of the key mathematical and statistical concepts and techniques which underpin mechanisms in Data Science and AI.

*Apply mathematical and statistical methods in these fields to help in the decision-making process.

*Critically evaluate the use of statistical analysis and the numeric interpretation of results as aids in the decision-making process.

*Critically appraise and present results of a statistical analysis to a diverse audience.

### Collaborative Discussion 1

**Imagine you have submitted a paper for publication (see Brown, 1994 in this week‚Äôs reading list). The editor has returned his comments saying that he is willing to accept the paper for publication on condition that Table 2 (see Brown, 1994) is changed. He feels that ‚Äòit is difficult for the reader to understand, contains too much information and is too large.‚Äô  You now have to redo the table 2 and present some of the findings using plots and present your results in the forum. Reflect also upon your experiences of undertaking this task and what you have learnt.**

**Initial Post**

Let me open this discussion forum with my experience when dealing with the activity of Unit 5 and in particular with the Table 2 of the article.  

Table 2 is quite informative as to the opinions of the general practitioners in Nottinghamshire on intrapartum care.  However, it is too cumbersome to read and not so easy to understand.  My approach would be quite different.  Let me describe to you how I reflected on this.  

Table 2 refers to 24 statements to which general practitioners have expressed their agreement/disagreement/no opinion.  At first, I thought it would be a good idea to take those statements with a rate of approval above 50% to showcase the findings of the research.  However, the statements which had got above the threshold of 50% were 11, so it would not make much difference for the visualisation of the data.  

Then, I chose the statements which equal to or go above 75% of the approval.  I found that 5 statements had got this percentage.   To illustrate the 5 most common opinions, I chose the horizontal bar chart (please see below) as I think it better highlights the message of the research.  Nevertheless, for the sake of the research I would put the existing Table 2 in the annex, so a more interested reader might be able to find more information.

![image](https://github.com/user-attachments/assets/a92d0780-6d7c-4852-bcde-b45de2d0f9d4)


In addition, I would also choose a couple of statements which did not get the threshold of 75% but they are quite popular among practitioners and would try to make my point by illustrating the whole spectrum of responses (Agree/Disagree/No opinion).  For example, I would take the statement on the fear of litigation and make a 'doughnut' (please see below) to highlight that only 20% of the practitioners seemed not to be afraid of litigation.  

![image](https://github.com/user-attachments/assets/d742e33a-0d40-4cc5-aef3-73009221e99e)


In brief, to better highlight my arguments, I would choose the most popular statements and create a graph (preferably a bar chart or a pie chart).  I think that for the abovementioned case boxplots, scatter plots, line graphs etc would not be suitable.  

I would also choose a couple of interesting statements that I would like to refer to in my analysis and try to present the percentages of Agree/Disagree/No opinion, using a pie chart/doughnut.  I would go for the simplicity as the simplest plots/graphs seem to be effective in conveying the message and are easy-to understand for the reader.


**Summary Post**

To provide a good visualisation of the Table 2, I took a different approach from the one taken by the authors of the article.  As Table 2 refers to 24 statements to which general practitioners have expressed their agreement/disagreement/no opinion, I thought it would be a good idea to take only those statements with a rate of approval above 75% to better showcase the findings of the research.   

I found that 5 statements had got this percentage.   To illustrate the 5 most common opinions, I chose the horizontal bar chart as I think it better highlights the message of the research.  Nevertheless, for the sake of the research I would put the existing Table 2 in the annex, so a more interested reader might be able to find more information.

In addition, I would also choose a couple of statements which did not get the threshold of 75% but they are quite popular among practitioners and would try to make my point by illustrating the whole spectrum of responses (Agree/Disagree/No opinion).  For example, I would take the statement on the fear of litigation and make a 'doughnut' to highlight that only 20% of the practitioners seemed not to be afraid of litigation.  

In brief, to better highlight my arguments, I would choose the most popular statements and create a graph (preferably a bar chart or a pie chart).  I think that for the abovementioned case boxplots, scatter plots, line graphs etc would not be suitable.  

I would also choose a couple of interesting statements that I would like to refer to in my analysis and try to present the percentages of Agree/Disagree/No opinion, using a pie chart/doughnut.  I would go for the simplicity as the simplest plots/graphs seem to be effective in conveying the message and are easy-to understand for the reader.

My peers with whom I had the opportunity to exchange some ideas tested different methods equally valid and effective.  In brief, Elias opted for using AI-enabled tools to make a very easy to read graph which was quite nice to see.  As for Jaco, I particularly liked the approach of keeping the 'No opinion' option in the graph as according to him, this piece of information is quite worth telling.  For example, if practitioners overwhelmingly decline to answer a question, then the researcher has to understand the reason behind this behaviour.  So, both methods developed by my two peers are worth keeping in mind.  

As for the inclusion or not of the ‚ÄòNo opinion‚Äô answers when calculating the percentages, some researchers opt for discarding them and choose to measure only the valid ones ‚ÄòAgree‚Äô and ‚ÄòDisagree‚Äô.  Nevertheless, there are some others who prefer to give a full account of the 'Agree', 'Disagree' and No opinion' answer options and they calculate the percentage of all these options.  In my opinion, both approaches can be used depending on the findings of the research and on whether we wish to give a detailed or less detailed account of the data we have collected.

To sum up, I opted for simplicity by highlighting only the most popular statements and by putting the whole table with all the details in an annex.  Another method was to provide very descriptive graph with vivid colours to highlight the differences of popularity among practitioners of the 24 statements.  Last but not least, it could be another option to focus on the 'Agree' rate and the rate of the 'No opinion' as this aspect could be very informative for our understanding of the research findings.  

In all the abovementioned cases, after the graphs are provided, it depends on the storyteller to frame the message they want to convey to the readers.  So, I think that the visualisation may at the end of the day help the storyteller develop a certain analysis and narrative for their research.

### Collaborative Discussion 2

**Discussion Topic**

**When dealing with statistical data, particularly for decision-making purposes, misinterpretations can be a serious issue.  For this collaborative discussion, read the Greenland et al. (2016) article from this week‚Äôs reading and reflect on these questions:  Under what situations should confidence intervals not be reported?  Why is a p-value not always enough to report?**

A lower than 0.05 (5%) p-value, as we have learned in this Module, enables us to reject the null hypothesis and assume that the test hypothesis is valid.  Conversely, a p-value higher than 0.05 (5%) lead us to the conclusion that the null hypothesis cannot be rejected and so the sample differences may be attributed to chance.   

In this regard, Ranstam (2012) argues that 'statistically insignificant outcome indicates nothing more than that the observed sample is too small to detect a population effect.  A statistically insignificant outcome should be interpreted as ‚Äúabsence of evidence, not evidence of absence‚Äù.  In other words, if a p-value is greater than the arbitrary cut-off of 5%, we lack evidence to assume that the test hypothesis is valid, but we cannot say with certainty that the null hypothesis is true.  According to Ranstam (2012), '[a] P-value provides only uncertainty information vis-a-vis a specific null hypothesis, no information on the statistical precision of an estimate.

'The smaller the P value, the more unusual the data would be if every single assumption were correct; but a very small P value does not tell us which assumption is incorrect'.  'Not only does a P value not tell us whether the hypothesis targeted for testing is true or not; it says nothing specifically related to that hypothesis unless we can be completely assured that every other assumption used for its computation is correct‚Äîan assurance that is lacking in far too many studies' (Greenland et al., 2016)

Sullivan & Feinn (2012) add another interesting dimension when dealing with and interpreting p-value and confidence intervals, that of the effect size.  'The effect size is the main finding of a quantitative study. While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect.  In reporting and interpreting studies, both the substantive significance (effect size) and statistical significance (P value) are essential results to be reported. 

This means that it is not enough to only report the p-value as this could be misleading and not informative as for our research.


**Bibliography**

Greenland, S., Senn, SJ., Rothman, K. J., Carlin, B., Poole, C., Goodman, N., & Altman, G. (2016) Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. European journal of epidemiology 31(4). 337‚Äì350. 

Ranstam, J. (2012) Why the P-value culture is bad and confidence intervals a better alternative. Osteoarthritis and Cartilage Volume 20, Issue 8. 805-808. https://doi.org/10.1016/j.joca.2012.04.001

Sullivan G.M., Feinn R. (2012) Using Effect Size‚Äîor Why the P Value Is Not Enough. Journal of Graduate Medical  Education Available on https://online225.psych.wisc.edu/wp-content/uploads/225-Master/225-UnitPages/Unit-07/Sullivan_JGME_2012.pdf [Accessed on 17 December 2024]


### End of Module 2 Assessment: Individual Reflection

**Assignment topic**

**This assessment focuses on your reflections on the skill and knowledge gained from the module. Please note that your reflections are not seen by other students, so you are free to discuss what is relevant to your learning and the processes you have gone through. Your reflection should address the following: Reflect in your data analysis skill and statistical knowledge and discuss how confident you are using R for statistical data analysis. Mention any challenges you faced.  Reflect upon your experience of interpreting statistical findings using p-value and confidence interval.  Also reflect upon on producing summary tables, contingency tables and producing plots using R.**

The Numerical Analysis module is part of the MSc programme, I am following at the University of Essex Online, in the field of Artificial Intelligence.   The aim of this module is for postgraduate students to: ‚Äòa) develop a systematic understanding of foundational mathematical principles and methods, b) facilitate their ability to interpret the results generated when using data science and AI tools, c) critically evaluate the use of statistical analysis and the numeric interpretation of results as aids in the decision-making process and d) critically appraise and present results of a statistical analysis to a diverse audience‚Äô (UoEO Module Home, n.d.).  A prominent role in achieving these objectives plays the familiarisation of the postgraduate student with the RStudio, which makes statistical data analysis and representation of analysis results (plots, graphs) of large datasets much easier and faster.

For this individual reflection, I will make use of mainly two reflective models: 
a) ‚ÄòWhat? So What? Now what?‚Äô (experience, implication of the situation and action plan) (Driscol, 1994 and Rolfe et al., 2001) and 
b) the Gibbs‚Äô reflective Cycle (description, feelings, evaluation, analysis, conclusion, action plan) (UoEO Short Guide to Reflective Writing, n.d. and The University of Edinburgh Reflection Toolkit, n.d.).  

According to the structure of the module, mathematical concepts and foundations of statistical data analysis were first presented on the learning platform either via lecturecasts, unit notes, or reading lists.  Further, we were tasked to perform RStudio commands to get a certain given result (by following the recommended material instructions).  Then, we were supposed to perform several formative tasks and data activities, to familiarise ourselves with the software and learn how to formulate commands (coding process) and get data analysis results by ‚Äòtrial and error‚Äô.
Given that I do not come from a STEM background myself, having to deal with mathematical concepts and statistical foundations was quite overwhelming for me, not to say a daunting task.  However, I was aware that some basic concepts of statistical analysis would be very helpful for me to advance with the following modules in the Artificial Intelligence programme.  What I have learned in this module, I believe will largely help me interpret statistical analysis results (assessing the p-value, the Confidence Interval (CI), the correlation co-efficient etc), be critical about the robustness of the findings and evaluate the usefulness of the statistical results as aids to decision-making process.

I enjoyed the first introductory units of the module very much, where basic mathematical and statistical concepts were effectively explained such as mean/median, minimum/maximum/range, variance/standard deviation, the difference between descriptive and inferential statistics etc.  In this effort of mine, my principal ally was the unit notes, the recommended reading lists, the seminar sessions and the activities we were asked to perform later.  The pace of the learning was normal, at least in the first half of the module, but it acquired much greater difficulty towards the end.  

In the last units, it has to be borne in mind that the lecturecasts were very difficult to understand, as they were intended for a more expert audience or at least to those who were already knowledgeable.  This is why I got to watch them again and again and, in particular, after having read the recommended bibliography, watched YouTube videos and visited informative websites on statistics and RStudio (Annex).   I was quite determined to get a grasp of all items to be covered in the module, so I made use of every available reliable source apart from the module material (Annex).  I also exchanged a series of emails with my tutor who kindly gave me feedback on the commands I had performed on RStudio and some hints when I needed some extra help.  
Another formative activity that also contributed to my learning process was engaging in a discussion with my peers (Collaborative discussions 1 & 2). Even though, due to the non-mandatory nature of the activity, not all peers contributed to the discussion, I exchanged valuable opinions with those who participated.  Having to draft a post and respond to the comments of the peers, made me read other material than that offered in the module, from the University of Essex online library and the Google Scholar.  Writing a post in a collaborative discussion forum may be work-intensive, as each postgraduate student is keen to share an impactful contribution, which will generate interest (Annex).

Nevertheless, the trickiest part for my learning process was performing commands and getting statistical results on RStudio.  This seemed to me quite frustrating at some point, as the absence of a bracket ‚Äò()‚Äô or the superfluous addition of a character made the software giving an ‚Äòerror‚Äô.  In many cases, it was not clear to me where the error was lying.  This is why I had to repeat many times the commands, making sure that all components were included, ask guidance from my tutor, and seek help from reliable YouTube videos and websites.  After having completed the module, I now feel much more confident with at least the basic commands of the software such as performing basic arithmetic, converting variables, and managing missing variables, developing graphs and plots, creating sub samples, performing statistical analysis using a null or alternative hypothesis, taking a decision based on p-value and statistical significance and running normality tests, correlations tests, and regression analysis.

Familiarising myself with RStudio and statistical analysis made me realise how important it is to be able to understand if the result I get is statistically significant, if I can reject the null hypothesis and accept the alternative one (when the p-value is lower than 0.05), how to interpret the Confidence Interval etc  This will help me get an insight into whether the statistical analysis I read is reliable or not, so I can make an evidence-informed decision.  I am sure that the valuable knowledge gained in this module will help me understand better how the Artificial Intelligence is working as well as its potential and opportunities but also its risks and implications in our lives.

To conclude, I got to understand how important it is to learn by experience and by ‚Äòtrial and error‚Äô, especially when having to learn mathematical and statistical analysis concepts and perform RStudio commands.  While the feeling of frustration when you do not get the desired result or when you find it difficult to grasp a certain concept is overwhelming, on the other hand, it is equally rewarding when you are patient, read very carefully the material, ask the tutor for guidance, follow the seminar sessions, watch YouTube videos, seek other reliable sources and finally when you deepen your knowledge and reach the desired objective.
 
**List of References**

Driscoll J. (1994). Reflective practice for practise. Senior Nurse, 13, 47 -50

Rolfe, G., Freshwater, D. & Jasper, M. (2001) Critical reflection in nursing and the helping professions: a user‚Äôs guide. Basingstoke: Palgrave Macmillan 

The University of Edinburgh (n.d.). Reflection Toolkit. Available from https://reflection.ed.ac.uk/reflectors-toolkit/reflecting-on-experience [Accessed on 9 January 2025]

UoEO (n.d.). Numerical Analysis October 2024, Module Home Available from https://www.my-course.co.uk/course/view.php?id=12569&section=0 [Accessed on 12 January 2025]

UoEO (n.d.).  Short Guide to Reflective Writing. Available from A short guide to reflective writing.pdf [Accessed on 9 January]

UoEO (n.d.).  Study Skills Hub. Reflective Writing Video. Available from https://www.my-course.co.uk/course/view.php?id=13&section=9#reflectivewriting [Accessed on 6 January]


## Machine-Learning
This is my learning experience for Module 3 of the MSc Artificial Intelligence

### In the module Machine Learning I shall:

*Learn about the key paradigms and algorithms in machine learning.

*Get an understanding of data analytics based on machine learning and using modern programming tools, such as Python or R.

*Experience how machine learning and data analytics can be used in real-world applications.

*Acquire the ability to gather and synthesise information from multiple sources to aid in the systematic analysis of complex problems using machine learning tools and algorithms.

### On completion of the module Machine Learning, I will be able to:

*Articulate the legal, social, ethical, and professional issues faced by machine learning professionals.

*Understand the applicability and challenges associated with different datasets for the use of machine learning algorithms.

*Apply and critically appraise machine learning techniques to real-world problems, particularly where technical risk and uncertainty is involved.

*Systematically develop and implement the skills required to be effective member of a development team in a virtual professional environment, adopting real-life perspectives on team roles and organisation.

### Collaborative Discussion 1

### Instructions

**Identify a specific incident (not covered in your reading list) where the failure of an information system has had a significant impact.  Your post could consider a range of impacts of the failure, including: the implications to customers, the economic cost, the reputational cost, or any other relevant impacts.**

‚ÄòSystem failure is a system that fails to develop or does so in a stunted fashion‚Äô (Carlsson and Jacobsson ,1997 in Bergek et al., 2008).   The first six months of 2018, the national IT systems across the NHS in Wales experienced such system failures as a total of 21 outages took place resulting in various disruptions for GPs, health care staff and patients (Postelnicu, 2018).   

These IT system failures in the Welsh NHS affected appointments, prescriptions and in general the health care provision as the professionals were unable to access electronic patients records for several hours (Donnely, 2018).   Moreover, patients could not book or re-book or cancel their appointments, nor be given test results, whereas GPs could not have access to blood and X-Ray results (BBC, 2018).  Dr Peter Saul, of the Royal College of GPs in Wales, said: "Today, IT systems are as critical to clinicians as stethoscopes and scanners. Data outages can be extraordinarily disruptive for practices and for patients‚Äô (Geraint, 2018).

As Schwab (2016) pointed out the Fourth Industrial Revolution, we are now experiencing, is a distinct one compared to the previous as for three components: velocity, scope, and systems impact.  System interdependencies can result in disrupting daily life and industries in every country.  On top of that, there may be a widespread feeling among the population that the extensive availability and circulation of high volumes of both structured and unstructured data has a negative impact on our inner lives and therefore we may lose control over them.
By interdependency is meant ‚Äòa bidirectional relationship between two infrastructures through which the state of each infrastructure influences or is correlated to the state of the other‚Äô.  This is why, ‚Äò[‚Ä¶] identifying, understanding, and analysing such interdependencies are significant challenges‚Äô, exacerbated by the breadth and complexity of critical IT nationwide infrastructures (Rinaldi et al., 2001).

The Welsh NHS IT system failures had to do with infrastructures which share cyber interdependency (Rinaldi et al., 2001), meaning that their state depends on information transmitted through the information infrastructure.  Eventually, such weaknesses in system structure may lead to interaction failures (related to networks) and to institutional failures (related to institutions) as it was the case of the Welsh NHS outages and may have an impact on the public trust towards state actors and operators.

Realising to what extent governments and public or private databases are vulnerable to failures or to external threats is crucial for how public policies will take shape and public goods will be delivered to citizens in the future.  Concerns about the vulnerability may even make countries roll back their initial plans for full replacement of the physical dimension into the digital one.  ‚ÄòSweden and Norway are backpedalling on plans for cashless societies over fears that fully digital payment systems would leave them vulnerable to Russian security threats, and concern for those unable to use them.  Prolonged power cuts, system failures or digital attacks on payment systems and banks could leave cash as the only alternative that is easily available‚Äô (Bryant, 2024).

### Bibliography

BBC (24 January 2018), Welsh NHS systems back up after computer 'chaos', Available at https://www.bbc.com/news/uk-wales-42803118 [Accessed on 31 January 2025]

Bergek A., Jacobsson S., Carlsson B., Lindmark S., Rickne A. (2008), Analyzing the functional dynamics of technological innovation systems: A scheme of analysis, Research Policy, Volume 37, Issue 3, 2008, Pages 407-429, https://doi.org/10.1016/j.respol.2007.12.003.

Bryant, M. (30 October 2024) Sweden and Norway rethink cashless society plans over Russia security fears, The Guardian, Available at https://www.theguardian.com/world/2024/oct/30/sweden-and-norway-rethink-cashless-society-plans-over-russia-security-fears [Accessed on 29 January 2025]

Donnelly, C. (25 January 2018), NHS Wales IT outage: What went wrong with its datacentres?, ComputerWeekly.com, Available at https://www.computerweekly.com/news/252433769/NHS-Wales-IT-outage-What-went-wrong-with-its-datacentres [Accessed on 31 January 2025]

Geraint, T. (8 November 2018), Outdated NHS Wales IT system 'needs a reboot', BBC Wales News, Available at https://www.bbc.com/news/uk-wales-46126048 [Accessed on 30 January 2025]
Postelnicu, L. (8 November 2018), HealthcareITNews, Report on informatics systems in NHS Wales raises "alarming findings", Available at https://www.healthcareitnews.com/news/emea/report-informatics-systems-nhs-wales-raises-alarming-findings [Accessed on 31 January 2025]

Rinaldi S. M., Peerenboom J. P. & Kelly T. K. (2001), "Identifying, understanding, and analyzing critical infrastructure interdependencies," IEEE Control Systems Magazine, vol. 21, no. 6, pp. 11-25, December 2001, doi: 10.1109/37.969131.

Schwab, K. (2016) The Fourth Industrial Revolution: What it means and how to respond. World Economic Forum. Available at https://www.weforum.org/stories/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-to-respond [Accessed: 29 January 2025].


<!---
narchondas/narchondas is a ‚ú® special ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
